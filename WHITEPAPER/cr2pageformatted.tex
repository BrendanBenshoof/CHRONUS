%%%%%%%%%%%%%%%%%%%%%%
\documentclass{singlecol-new}
%%%%%%%%%%%%%%%%%%%%%%

\usepackage[round]{natbib}
\usepackage{stfloats}
\usepackage{mathrsfs,amsmath,upgreek}
\usepackage{wrapfig}
\def\newblock{\hskip .11em plus .33em minus .07em}

\newcommand{\be}{\begin{eqnarray}}
\newcommand{\ee}{\end{eqnarray}}
\newcommand{\nn}{\nonumber}

\theoremstyle{TH}{
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{corrolary}[lemma]{Corrolary}
\newtheorem{conjecture}[lemma]{Conjecture}
\newtheorem{proposition}[lemma]{Proposition}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{stheorem}[lemma]{Wrong Theorem}
\newtheorem{algorithm}{Algorithm}
}

\theoremstyle{THrm}{
\newtheorem{definition}{Definition}
\newtheorem{question}{Question}
\newtheorem{remark}{Remark}
\newtheorem{scheme}{Scheme}
}

\theoremstyle{THhit}{
\newtheorem{case}{Case}[section]
}

\makeatletter

\def\Reals{\mathbb{R}}
\def\Ints{\mathbb{Z}}
\def\Nats{\mathbb{N}}

\def\theequation{\arabic{equation}}

\def\tc{\textcolor{red}}

\def\BottomCatch{%
\vskip -10pt
\thispagestyle{empty}%
\begin{table}[b]%
\NINE\begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}lcr@{}}%
\\[-12pt]
Copyright \copyright\ 2012 Inderscience Enterprises Ltd. & &%
\end{tabular*}%
\vskip -30pt%
%%\vskip -35pt%
\end{table}%
%\def\tc{\textcolor}{red}
} \makeatother
\linespread{0.95}
\setlength{\parskip}{0cm}
\setlength{\parindent}{1em}
\usepackage[compact]{titlesec}
\titlespacing{\section}{0pt}{2ex}{1ex}
\titlespacing{\subsection}{0pt}{1ex}{0ex}
\titlespacing{\subsubsection}{0pt}{0.5ex}{0ex}

%%%%%%%%%%%%%%%%%
\begin{document}%
%%%%%%%%%%%%%%%%%

\thispagestyle{plain}

\setcounter{page}{1}

%\LRH{xxxx}

%\RRH{xxxx}

%\VOL{x}

%\ISSUE{x}

%\PUBYEAR{xxxx}

%\BottomCatch

%\CLline

%\subtitle{}

\title{MapReduce on a Chord Distributed Hash Table}

\author{ }

%\affA{xxxx*\\*Corresponding author}

%\authorB{xxxx}
%\affB{xxxx}

\begin{abstract}

MapReduce platforms are designed for datacenters, which are highly centralized in nature.  These platforms require a centralized source to maintain and coordinate the network, which leads to a single point of failure.  We present ChordReduce, a generalized and completely decentralized MapReduce framework which utilizes the peer-to-peer protocol Chord.  ChordReduce's robustness, scalability, and lack of a single point of failure allows MapReduce to be easily deployed in a greater variety of contexts, including cloud and loosely coupled environments.

\end{abstract}

\KEYWORD{MapReduce; P2P; Parallel Processing; Peer-to-Peer Computing; Cloud Computing; Middleware;}
\vspace{-1in}
\maketitle

\section{Introduction}
\label{sec-introduction}
Google's MapReduce \citep{mapreduce} paradigm has rapidly become an integral part in the world of data processing.
Popular platforms for MapReduce, such as Hadoop, are explicitly designed to be used in large datacenters.  
A consequence of this is the analysis and optimization of MapReduce has largely remained constrained to that context.  
We were motivated to create a MapReduce framework that could be easily deployed in other contexts, such as cloud computing or peer-to-peer networks, with minimal configuration.
A more general-use framework for MapReduce cannot make the assumptions that other frameworks rely on, such as a high performance infrastructure, a static network, or a centralized source to organize and coordinate the network and its resources.  

We present ChordReduce, our MapReduce framework.  It is highly robust, scalable, and does not require a centralized source of coordination.  ChordReduce is completely decentralized, avoiding the single points of failure that exist in Hadoop and other frameworks. Responsibilities  that required a central coordinator, such as handling metadata, coordinating the execution of MapReduce, or assigning backups are all handled by the underlying Chord protocol.


\section{ChordReduce}

ChordReduce is built on top of Chord \citep{Chord}, a peer-to-peer (P2P) networking protocol for distributed storage and file sharing that provides $\log(n)$ lookup time for any particular file or node.
Files and nodes are evenly distributed across a ring overlay and organized so the responsibilities of a failed node are automatically reassigned. 

We leverage Chord's properties to distribute Map and Reduce tasks evenly among nodes and maintain a high degree of robustness during execution.  
The loss of a single node or a group of nodes during execution does not impact the soundness of the results and their tasks are automatically reassigned. 
An additional benefit is nodes which join the ring during runtime can automatically have work distributed to them.

MapReduce frameworks are generally hierarchical, with a centralized source responsible for scheduling work, creating and placing backups of data, or the status of nodes.  This leads centralized MapReduce implementations to having a single point of failure.

Our framework allows us not only to distribute the data and tasks among members of the network, but distribute the responsibility of maintaining the network as well, removing the need for any centralized coordinator.
ChordReduce provides ideal characteristics for a MapReduce framework by being highly scalable, fault-tolerant even during execution, and minimizes the work needed to maintain the network.  The lack of a single point of failure allows ChordReduce to be deployed in a greater variety of environments and contexts, ranging from datacenters, P2P networks, or cloud computing.




\section{Implementation and Experiments}

We implemented ChordReduce by building our own version of Chord plus a module to run MapReduce operations over the network. Files are split into blocks and stored on the network using our implementation of CFS \citep{CFS}.  File splitting can be user defined or handled automatically by ChordReduce.  When the user wants to run a MapReduce job over a file or files, a task is sent to each node storing a block of that file.  In the case of computations that are purely mathematical, tasks are uniformly distributed across the network.

\begin{wraptable}{r}{0.5\textwidth}\footnotesize
	\caption{\footnotesize{The results of calculating $\pi$ by generating $10^8$ samples under churn. Churn is the chance for each node to join or leave the network. The large speedup is from joining nodes acquiring work during experimental runtime.}}    
    \begin{tabular}{|r|r|r|}
        \hline 
        Churn per second & Average runtime (s) & Speedup vs 0\% \\ \hline{}
        0\% & 441.57 & 1.00 \\ \hline
        0.00250\% & 331.80  &  1.24 \\ \hline
        0.00775\%  & 445.47 & 0.92 \\ \hline
        0.025\% & 431.86 & 0.95 \\ \hline 
        0.4\% & 329.20 & 1.25 \\ \hline
        0.8\% & 191.25 & 2.15 \\ \hline 
    \end{tabular}
    
    \label{churnSpeed}
\end{wraptable}
For our experiments, we wanted to establish that the speedup from distributing the work would follow an expected logarithmic pattern, even under differing rates of churn.  We implemented two MapReduce jobs for experiments: a Monte-Carlo approximation of $\pi$ and creating a word frequency list on a document.  Our experiments varied the rate of churn the network experienced, the number of workers in the network, the number of tasks distributed among the nodes, and the frequency at which maintenance occurred. Table \ref{churnSpeed} shows the experimental results for calculating $\pi$ under different rates of churn in an initial ring of 40 deployed nodes.  Our results showed that performance did not suffer under even very high rates of churn and that the reassignment of work at runtime to joining nodes allowed the network to perform better in some cases.



\section{Conclusion}
Our experiments establish that ChordReduce operates even under extremely high rates of churn and follows the desired logarithmic speedup.  
These results establish that ChordReduce is an efficient implementation of MapReduce. 
The applications are far-reaching, especially for big data problems and those that are massively parallel. 
Implementing MapReduce on a Chord peer-to-peer network demonstrates that the Chord network is an excellent platform for distributed and concurrent programming in cloud and loosely coupled environments.

{\footnotesize
\bibliographystyle{plainnat}
\bibliography{CHRONUS}
}






%\begin{thebibliography}{99}

%\bibitem[\protect\citeauthoryear{xxxx}{2011}]{Zhang11}
%xxxx (2011) `Reliable packets delivery over segment-based multi-path
%in wireless ad hoc networks', {\it Pervasive Computing and
%Applications (ICPCA), 2011 6th International Conference on},
%pp.300-–306. \tc{AUTHOR PLEASE SUPPLY LOCATION.}

%\end{thebibliography}


\end{document}

%\def\notesname{Note}
%
%\theendnotes

%\section*{Query}
%
%\tc{AQ1: AUTHOR PLEASE CITE FIGURE 8 IN TEXT.}

