\documentclass{article}


%%% PAGE DIMENSIONS
\usepackage{geometry} % to change the page dimensions
\geometry{letterpaper} % or letterpaper (US) or a5paper or....
% \geometry{margins=2in} % for example, change the margins to 2 inches all round
% \geometry{landscape} % set up the page for landscape
%   read geometry.pdf for detailed page layout information

\usepackage{graphicx} % support the \includegraphics command and options

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES
\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)
%\usepackage{listings}
%\usepackage[scaled]{beramono}
%\usepackage[T1]{fontenc}
\usepackage{xcolor}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
\usepackage{wrapfig}
% These packages are all incorporated in the memoir class to one degree or another...
%\usepackage{algorithm}
%\usepackage{algpseudocode}
%\usepackage{amsmath}
%\usepackage{amsthm}

%%% HEADERS & FOOTERS
%\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
%\pagestyle{fancy} % options: empty , plain , fancy
%%\renewcommand{\headrulewidth}{0pt} % customise the layout...
%\lhead{}\chead{}\rhead{}
%\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
%\usepackage{sectsty}
%\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
%\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
%\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
%\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
%\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

%%% END Article customizations

%%% The "real" document content comes below...

\title{MapReduce on a Chord Distributed Hash Table}
%\author{\IEEEauthorblockN{Andrew Rosen \qquad Brendan Benshoof \qquad Robert W. Harrison \qquad Anu G. Bourgeois}


%\author{
%Andrew Rosen \qquad Brendan Benshoof \qquad Matt Erwin \qquad Robert Harrison \qquad Anu Bourgeois  \\Department of Computer Science, Georgia State University\\ 34 Peachtree St NW \\ Atlanta, Georgia 30303\\  rosen@cs.gsu.edu }
\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 


\hyphenation{op-tical net-works semi-conduc-tor Chord-Reduce Map-Reduce Data-Nodes Name-Nodes}

\begin{document}
%\lstset{language=Python, basicstyle= \footnotesize\ttfamily ,showstringspaces=false, frame=single, commentstyle=\itshape\color{gray}, identifierstyle=\color{black},  keywordstyle=\bfseries\color{red!40!black}, stringstyle=\color{blue}} 

\maketitle
\vspace*{-0.65in}
\begin{abstract}

MapReduce platforms are designed for datacenters, which are highly centralized in nature.  These platforms require a centralized source to maintain and coordinate the network, which leads to a single point of failure.  We present ChordReduce, a generalized and completely decentralized MapReduce framework which utilizes the peer-to-peer protocol Chord.  ChordReduce's robustness, scalability, and lack of a single point of failure allows MapReduce to be easily deployed in a greater variety of contexts, including cloud and loosely coupled environments.


\end{abstract}


%\begin{IEEEkeywords}
%MapReduce; P2P; Parallel Processing; Peer-to-Peer Computing; Cloud Computing; Middleware;

%\end{IEEEkeywords}

\subsubsection*{Introduction}
%Google's MapReduce \cite{mapreduce} paradigm has rapidly become an integral part in the world of data processing and is capable of efficiently executing numerous Big Data programming and data-reduction tasks.  
%Using MapReduce, a problem can be split it into small, functionally identical tasks whose results can be combined into a single answer. 
%This approach can be applied to multiple problems, such as distributed sorting and creating an inverted index \cite{mapreduce}, making it extremely powerful and versatile.

Google's MapReduce \cite{mapreduce} paradigm has rapidly become an integral part in the world of data processing.
Popular platforms for MapReduce, such as Hadoop \cite{Hadoop}, are explicitly designed to be used in large datacenters.  
A consequence of this is the analysis and optimization of MapReduce has largely remained constrained to that context.  
We were motivated to create a MapReduce framework that could be easily deployed in other contexts, such as cloud computing or peer-to-peer networks, with minimal configuration.
A more general-use framework for MapReduce cannot make the assumptions that other frameworks rely on, such as a high performance infrastructure, a static network, or a centralized source to organize and coordinate the network and its resources.  

We present ChordReduce, our MapReduce framework.  It is highly robust, scalable, and does not require a centralized source of coordination.  ChordReduce is completely decentralized, avoiding the single points of failure that exist in Hadoop and other frameworks. Responsibilities  that required a central coordinator, such as handling metadata, coordinating the execution of MapReduce, or assigning backups,  are all handled by the underlying Chord protocol.

\subsubsection*{ChordReduce}

ChordReduce is built on top of Chord \cite{Chord}, a peer-to-peer (P2P) networking protocol for distributed storage and file sharing that provides $\log(n)$ lookup time for any particular file or node.
Files and nodes are evenly distributed across a ring overlay and organized so the responsibilities of a failed node are automatically reassigned. 

We leverage Chord's properties to distribute Map and Reduce tasks evenly among nodes and maintain a high degree of robustness during execution.  
The loss of a single node or a group of nodes during execution does not impact the soundness of the results and their tasks are automatically reassigned. 
An additional benefit is nodes which join the ring during runtime can automatically have work distributed to them.

MapReduce frameworks are generally hierarchical, with a centralized source responsible for scheduling work, creating and placing backups of data, or the status of nodes.  This leads centralized MapReduce implementations to having a single point of failure.

Our framework allows us not only to distribute the data and tasks among members of the network, but distributes the responsibility of maintaining the network as well, removing the need for any centralized point of failure from the network.
ChordReduce provides ideal characteristics for a MapReduce framework by being highly scalable, fault-tolerant even during execution, and minimizes the work need to maintain the network.  The lack of a single point of failure allows ChordReduce to be deployed in a greater variety of environments and contexts, ranging from datacenters, P2P networks, or cloud computing.




\subsubsection*{Implementation and Experiments}
\begin{wraptable}{r}{0.5\textwidth}\footnotesize
	\caption{\footnotesize{The results of calculating $\pi$ generating $10^8$ samples under churn. Churn is the chance for each node to join or leave the network. The large speedup is from joining nodes acquiring work during experimental runtime.}}    
    \begin{tabular}{|r|r|r|}
        \hline 
        Churn per second & Average runtime (s) & Speedup vs 0\% \\ \hline{}
        0\% & 441.57 & 1.00 \\ \hline
        0.00250\% & 331.80  &  1.24 \\ \hline
        0.00775\%  & 445.47 & 0.92 \\ \hline
        0.025\% & 431.86 & 0.95 \\ \hline 
        0.4\% & 329.20 & 1.25 \\ \hline
        0.8\% & 191.25 & 2.15 \\ \hline 
    \end{tabular}
    
    \label{churnSpeed}
\end{wraptable}
We implemented ChordReduce by building our own version of Chord plus a module to run MapReduce operations over the network. Files are split into blocks and stored on the network using our implementation of CFS \cite{CFS}.  File splitting can be user defined or handled automatically by ChordReduce.  When the user wants to run a MapReduce job over a file or files, a task is sent to each node storing a block of that file.  In the case of computations that are purely mathematical, tasks are uniformly distributed across the network.


For our experiments, we wanted to establish that the speedup from distributing the work would follow an expected logarithmic pattern, even under differing rates of churn.  We implemented two MapReduce jobs for experiments: a Monte-Carlo approximation of $\pi$ and creating a word frequency list on a document.  Our experiments varied the rate of churn the network experienced, the number of workers in the network, the number of tasks distributed among the nodes, and the frequency at which maintenance occurred. Table \ref{churnSpeed} shows the experimental results for calculating $\pi$ different rates of churn in an initial ring of 40 deployed nodes.  Our results showed that performance did not suffer under even very high rates of churn and that the reassignment of work at runtime to joining nodes allowed the network to perform better in some cases.







\subsubsection*{Conclusion}
Our other experiments showed that ChordReduce operates even under extremely high rates of churn and followed the desired logarithmic speedup.  
These results establish that ChordReduce is an efficient implementation of MapReduce. 
The applications are far-reaching, especially for big data problems and those that are massively parallel. 
Implementing MapReduce on a Chord peer-to-peer network demonstrates that the Chord network is an excellent platform for distributed and concurrent programming in cloud and loosely coupled environments.

{\footnotesize
\bibliographystyle{plain}
\bibliography{CHRONUS}
}
\end{document}